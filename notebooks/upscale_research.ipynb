{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/JingyunLiang/SwinIR.git\n",
    "!wget https://github.com/JingyunLiang/SwinIR/releases/download/v0.0/003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x2_GAN.pth -P experiments/pretrained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/content/SwinIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--task', type=str, default='color_dn', help='classical_sr, lightweight_sr, real_sr, '\n",
    "#                                                                  'gray_dn, color_dn, jpeg_car, color_jpeg_car')\n",
    "# parser.add_argument('--scale', type=int, default=1, help='scale factor: 1, 2, 3, 4, 8') # 1 for dn and jpeg car\n",
    "# parser.add_argument('--noise', type=int, default=15, help='noise level: 15, 25, 50')\n",
    "# parser.add_argument('--jpeg', type=int, default=40, help='scale factor: 10, 20, 30, 40')\n",
    "# parser.add_argument('--training_patch_size', type=int, default=128, help='patch size used in training SwinIR. '\n",
    "#                                    'Just used to differentiate two different settings in Table 2 of the paper. '\n",
    "#                                    'Images are NOT tested patch by patch.')\n",
    "# parser.add_argument('--large_model', action='store_true', help='use large model, only provided for real image sr')\n",
    "# parser.add_argument('--model_path', type=str,\n",
    "#                     default='model_zoo/swinir/001_classicalSR_DIV2K_s48w8_SwinIR-M_x2.pth')\n",
    "# parser.add_argument('--folder_lq', type=str, default=None, help='input low-quality test image folder')\n",
    "# parser.add_argument('--folder_gt', type=str, default=None, help='input ground-truth test image folder')\n",
    "# parser.add_argument('--tile', type=int, default=None, help='Tile size, None for no tile during testing (testing as a whole)')\n",
    "# parser.add_argument('--tile_overlap', type=int, default=32, help='Overlapping of different tiles')\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import argparse\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import torch\n",
    "import requests\n",
    "\n",
    "from models.network_swinir import SwinIR as net\n",
    "from utils import util_calculate_psnr_ssim as util\n",
    "\n",
    "\n",
    "def main(\n",
    "    task=\"real_sr\",\n",
    "    scale=1,\n",
    "    folder_lq=None,\n",
    "    tile=None,\n",
    "    tile_overlap=32,\n",
    "    model_path=\"experiments/pretrained_models/003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x4_GAN.pth\",\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"loading model from {model_path}\")\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "        url = \"https://github.com/JingyunLiang/SwinIR/releases/download/v0.0/{}\".format(\n",
    "            os.path.basename(model_path)\n",
    "        )\n",
    "        r = requests.get(url, allow_redirects=True)\n",
    "        print(f\"downloading model {model_path}\")\n",
    "        open(model_path, \"wb\").write(r.content)\n",
    "\n",
    "    model = define_model(scale, model_path)\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "    # setup folder and path\n",
    "    folder, save_dir, border, window_size = setup(task, scale, folder_lq)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    psnr, ssim, psnr_y, ssim_y, psnrb, psnrb_y = 0, 0, 0, 0, 0, 0\n",
    "\n",
    "    for idx, path in enumerate(sorted(glob.glob(os.path.join(folder, \"*\")))):\n",
    "        # read image\n",
    "        imgname, img_lq = get_image_pair(folder_lq, path)  # image to HWC-BGR, float32\n",
    "\n",
    "        if img_lq is None:\n",
    "            img = Image.open(path)\n",
    "            img.save(os.path.join(save_dir, imgname + \".png\"))\n",
    "            continue\n",
    "\n",
    "        img_lq = np.transpose(\n",
    "            img_lq if img_lq.shape[2] == 1 else img_lq[:, :, [2, 1, 0]], (2, 0, 1)\n",
    "        )  # HCW-BGR to CHW-RGB\n",
    "        img_lq = (\n",
    "            torch.from_numpy(img_lq).float().unsqueeze(0).to(device)\n",
    "        )  # CHW-RGB to NCHW-RGB\n",
    "\n",
    "        # inference\n",
    "        with torch.no_grad():\n",
    "            # pad input image to be a multiple of window_size\n",
    "            _, _, h_old, w_old = img_lq.size()\n",
    "            h_pad = (h_old // window_size + 1) * window_size - h_old\n",
    "            w_pad = (w_old // window_size + 1) * window_size - w_old\n",
    "            img_lq = torch.cat([img_lq, torch.flip(img_lq, [2])], 2)[\n",
    "                :, :, : h_old + h_pad, :\n",
    "            ]\n",
    "            img_lq = torch.cat([img_lq, torch.flip(img_lq, [3])], 3)[\n",
    "                :, :, :, : w_old + w_pad\n",
    "            ]\n",
    "            output = test(img_lq, model, tile, tile_overlap, scale, window_size)\n",
    "            output = output[..., : h_old * scale, : w_old * scale]\n",
    "\n",
    "        # save image\n",
    "        output = output.data.squeeze().float().cpu().clamp_(0, 1).numpy()\n",
    "        if output.ndim == 3:\n",
    "            output = np.transpose(\n",
    "                output[[2, 1, 0], :, :], (1, 2, 0)\n",
    "            )  # CHW-RGB to HCW-BGR\n",
    "        output = (output * 255.0).round().astype(np.uint8)  # float32 to uint8\n",
    "        cv2.imwrite(os.path.join(save_dir, imgname + \".jpeg\"), output)\n",
    "\n",
    "        print(\"Testing {:d} {:20s}\".format(idx, imgname))\n",
    "\n",
    "\n",
    "def define_model(scale, model_path):\n",
    "    # use 'nearest+conv' to avoid block artifacts\n",
    "    model = net(\n",
    "        upscale=scale,\n",
    "        in_chans=3,\n",
    "        img_size=64,\n",
    "        window_size=8,\n",
    "        img_range=1.0,\n",
    "        depths=[6, 6, 6, 6, 6, 6],\n",
    "        embed_dim=180,\n",
    "        num_heads=[6, 6, 6, 6, 6, 6],\n",
    "        mlp_ratio=2,\n",
    "        upsampler=\"nearest+conv\",\n",
    "        resi_connection=\"1conv\",\n",
    "    )\n",
    "    param_key_g = \"params_ema\"\n",
    "\n",
    "    pretrained_model = torch.load(model_path)\n",
    "    model.load_state_dict(\n",
    "        pretrained_model[param_key_g]\n",
    "        if param_key_g in pretrained_model.keys()\n",
    "        else pretrained_model,\n",
    "        strict=True,\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def setup(task, scale, folder_lq):\n",
    "    save_dir = f\"results/swinir_{task}_x{scale}\"\n",
    "    folder = folder_lq\n",
    "    border = 0\n",
    "    window_size = 8\n",
    "\n",
    "    return folder, save_dir, border, window_size\n",
    "\n",
    "\n",
    "def get_image_pair(folder_lq, path):\n",
    "    (imgname, imgext) = os.path.splitext(os.path.basename(path))\n",
    "    if (\"ovar\" in imgname) or (\"foll\" in imgname):\n",
    "        return imgname, None\n",
    "\n",
    "    img_lq = cv2.imread(path, cv2.IMREAD_COLOR).astype(np.float32) / 255.0\n",
    "\n",
    "    return imgname, img_lq\n",
    "\n",
    "\n",
    "def test(img_lq, model, tile, tile_overlap, scale, window_size):\n",
    "    if tile is None:\n",
    "        # test the image as a whole\n",
    "        output = model(img_lq)\n",
    "    else:\n",
    "        # test the image tile by tile\n",
    "        b, c, h, w = img_lq.size()\n",
    "        tile = min(tile, h, w)\n",
    "        assert tile % window_size == 0, \"tile size should be a multiple of window_size\"\n",
    "        tile_overlap = tile_overlap\n",
    "        sf = scale\n",
    "\n",
    "        stride = tile - tile_overlap\n",
    "        h_idx_list = list(range(0, h - tile, stride)) + [h - tile]\n",
    "        w_idx_list = list(range(0, w - tile, stride)) + [w - tile]\n",
    "        E = torch.zeros(b, c, h * sf, w * sf).type_as(img_lq)\n",
    "        W = torch.zeros_like(E)\n",
    "\n",
    "        for h_idx in h_idx_list:\n",
    "            for w_idx in w_idx_list:\n",
    "                in_patch = img_lq[..., h_idx : h_idx + tile, w_idx : w_idx + tile]\n",
    "                out_patch = model(in_patch)\n",
    "                out_patch_mask = torch.ones_like(out_patch)\n",
    "\n",
    "                E[\n",
    "                    ...,\n",
    "                    h_idx * sf : (h_idx + tile) * sf,\n",
    "                    w_idx * sf : (w_idx + tile) * sf,\n",
    "                ].add_(out_patch)\n",
    "                W[\n",
    "                    ...,\n",
    "                    h_idx * sf : (h_idx + tile) * sf,\n",
    "                    w_idx * sf : (w_idx + tile) * sf,\n",
    "                ].add_(out_patch_mask)\n",
    "        output = E.div_(W)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(\n",
    "    task=\"real_sr\",\n",
    "    model_path=\"experiments/pretrained_models/003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x2_GAN.pth\",\n",
    "    folder_lq=\"train_clean/\",\n",
    "    scale=2,\n",
    ")\n",
    "\n",
    "main(\n",
    "    task=\"real_sr\",\n",
    "    model_path=\"experiments/pretrained_models/003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x2_GAN.pth\",\n",
    "    folder_lq=\"test_clean/\",\n",
    "    scale=2,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
